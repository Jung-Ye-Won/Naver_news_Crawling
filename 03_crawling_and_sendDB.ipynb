{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee8f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45a68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver as wd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ff3572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gram\\AppData\\Local\\Temp\\ipykernel_17732\\513909967.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"C:\\\\Users\\\\gram\\\\chromedriver.exe\", chrome_options=chrome_options)\n",
      "C:\\Users\\gram\\AppData\\Local\\Temp\\ipykernel_17732\\513909967.py:12: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(\"C:\\\\Users\\\\gram\\\\chromedriver.exe\", chrome_options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "#op = Options()\n",
    "#op.add_experimental_option('prefs',{'download.default_directory':r'C:\\Users\\gram\\samwon\\download'})\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-setuid-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_experimental_option('prefs',{'download.default_directory':r'C:\\Users\\gram\\samwon\\download'})\n",
    "\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\gram\\\\chromedriver.exe\", chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "year = datetime.now().year\n",
    "month = datetime.now().month\n",
    "end_day = datetime.now().day\n",
    "\n",
    "delta = timedelta(-1)\n",
    "start_date = datetime.now() + delta\n",
    "start_day = start_date.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e87cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(driver, crawl_date, press_list, title_list, link_list, date_list, more_news_base_url=None, more_news=False):\n",
    "    more_news_url_list = []\n",
    "    while True:    \n",
    "        page_html_source = driver.page_source\n",
    "        url_soup = BeautifulSoup(page_html_source, 'lxml')\n",
    "        \n",
    "        more_news_infos = url_soup.select('a.news_more')\n",
    "        \n",
    "        if more_news:\n",
    "            for more_news_info in more_news_infos:\n",
    "                more_news_url = f\"{more_news_base_url}{more_news_info.get('href')}\"\n",
    "\n",
    "                more_news_url_list.append(more_news_url)\n",
    "\n",
    "        article_infos = url_soup.select(\"div.news_area\")\n",
    "        \n",
    "        if not article_infos:\n",
    "            break\n",
    "\n",
    "        for article_info in article_infos:  \n",
    "            press_info = article_info.select_one(\"div.info_group > a.info.press\")\n",
    "            \n",
    "            if press_info is None:\n",
    "                press_info = article_info.select_one(\"div.info_group > span.info.press\")\n",
    "            article = article_info.select_one(\"a.news_tit\")\n",
    "            \n",
    "            press = press_info.text.replace(\"언론사 선정\", \"\")\n",
    "            title = article.get('title')\n",
    "            link = article.get('href')\n",
    "\n",
    "            press_list.append(press)\n",
    "            title_list.append(title)\n",
    "            link_list.append(link)\n",
    "            date_list.append(crawl_date)\n",
    "\n",
    "        time.sleep(2.0)\n",
    "                      \n",
    "        next_button_status = url_soup.select_one(\"a.btn_next\").get(\"aria-disabled\")\n",
    "        \n",
    "        if next_button_status == 'true':\n",
    "            break\n",
    "        \n",
    "        time.sleep(3.0)\n",
    "        \n",
    "        next_page_btn = driver.find_element(By.CSS_SELECTOR, \"a.btn_next\").click()\n",
    "    \n",
    "    return press_list, title_list, link_list, more_news_url_list\n",
    "    \n",
    "    \n",
    "\n",
    "def get_naver_news_info_from_selenium(keyword, save_path, target_date, ds_de, sort=0, remove_duplicate=False):\n",
    "    crawl_date = f\"{target_date[:4]}.{target_date[4:6]}.{target_date[6:]}\"\n",
    "    driver = wd.Chrome(\"C:\\\\Users\\\\gram\\\\chromedriver.exe\") # chromedriver 파일 경로\n",
    "\n",
    "    encoded_keyword = urllib.parse.quote(keyword)\n",
    "    url = f\"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort={sort}&photo=0&field=0&pd=3&ds={ds_de}&de={ds_de}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{target_date}to{target_date}&is_sug_officeid=0\"\n",
    "    \n",
    "    more_news_base_url = \"https://search.naver.com/search.naver\"\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    press_list, title_list, link_list, date_list, more_news_url_list = [], [], [], [], []\n",
    "    \n",
    "    press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
    "                                                                             crawl_date=crawl_date, \n",
    "                                                                             press_list=press_list, \n",
    "                                                                             title_list=title_list, \n",
    "                                                                             link_list=link_list,\n",
    "                                                                             date_list=date_list,\n",
    "                                                                             more_news_base_url=more_news_base_url,\n",
    "                                                                             more_news=True)\n",
    "    driver.close()\n",
    "    \n",
    "    if len(more_news_url_list) > 0:\n",
    "        print(len(more_news_url_list))\n",
    "        more_news_url_list = list(set(more_news_url_list))\n",
    "        print(f\"->{len(more_news_url_list)}\")\n",
    "        for more_news_url in more_news_url_list:\n",
    "            driver = wd.Chrome(\"C:\\\\Users\\\\gram\\\\chromedriver.exe\")\n",
    "            driver.get(more_news_url)\n",
    "            \n",
    "            press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
    "                                                                             crawl_date=crawl_date, \n",
    "                                                                             press_list=press_list, \n",
    "                                                                             title_list=title_list, \n",
    "                                                                             link_list=link_list,\n",
    "                                                                             date_list=date_list)\n",
    "            driver.close()\n",
    "    article_df = pd.DataFrame({\"날짜\": date_list, \"언론사\": press_list, \"제목\": title_list, \"링크\": link_list})\n",
    "    \n",
    "    print(f\"extract article num : {len(article_df)}\")\n",
    "    if remove_duplicate:\n",
    "        article_df = article_df.drop_duplicates(['링크'], keep='first')\n",
    "        print(f\"after remove duplicate -> {len(article_df)}\")\n",
    "    \n",
    "    article_df.to_excel(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7bb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def crawl_news_data(keyword, year, month, start_day, end_day, save_path):\n",
    "    for day in tqdm(range(start_day, end_day+1)):\n",
    "        date_time_obj = datetime(year=year, month=month, day=day)\n",
    "        target_date = date_time_obj.strftime(\"%Y%m%d\")\n",
    "        ds_de = date_time_obj.strftime(\"%Y.%m.%d\")\n",
    "\n",
    "        get_naver_news_info_from_selenium(keyword=keyword, save_path=f\"{save_path}/{keyword}/{target_date}_{keyword}_.xlsx\", target_date=target_date, ds_de=ds_de, remove_duplicate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6aaf19f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#keywords = ['틴더', '토스', '야놀자', '당근마켓']\n",
    "keywords = ['야놀자', '당근마켓']\n",
    "save_path = \"C:\\\\Users\\\\gram\\\\samwon\\\\naver_news_article\"\n",
    "\n",
    "#경로 있는지 확인 후 생성\n",
    "for keyword in keywords:\n",
    "    if os.path.isdir(f\"{save_path}/{keyword}\"):\n",
    "        print('have files')\n",
    "    else :\n",
    "        os.makedirs(f\"{save_path}/{keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1bbf7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_excel_files(file_path, file_format, save_path, save_format, columns=None):\n",
    "    merge_df = pd.DataFrame()\n",
    "    file_list = file_list = [f\"{file_path}/{file}\" for file in os.listdir(file_path) if file_format in file]\n",
    "    \n",
    "    for file in file_list:\n",
    "        if file_format == \".xlsx\":\n",
    "            file_df = pd.read_excel(file)\n",
    "        else:\n",
    "            file_df = pd.read_csv(file)\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = file_df.columns\n",
    "            \n",
    "        temp_df = pd.DataFrame(file_df, columns=columns)\n",
    "        \n",
    "        merge_df = merge_df.append(temp_df)\n",
    "        \n",
    "    if save_format == \".xlsx\":\n",
    "        merge_df.to_excel(save_path, index=False)\n",
    "    else:\n",
    "        merge_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca654fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gram\\AppData\\Local\\Temp\\ipykernel_17732\\4291397367.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  merge_df = merge_df.append(temp_df)\n",
      "C:\\Users\\gram\\AppData\\Local\\Temp\\ipykernel_17732\\4291397367.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  merge_df = merge_df.append(temp_df)\n",
      "C:\\Users\\gram\\AppData\\Local\\Temp\\ipykernel_17732\\4291397367.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  merge_df = merge_df.append(temp_df)\n",
      "C:\\Users\\gram\\AppData\\Local\\Temp\\ipykernel_17732\\4291397367.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  merge_df = merge_df.append(temp_df)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for keyword in keywords:\n",
    "    #print(f\"start keyword - {keyword} crawling ...\")\n",
    "\n",
    "    crawl_news_data(keyword=keyword, year=year, month=month, start_day=start_day, end_day=end_day, save_path=save_path)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        merge_excel_files(file_path=f\"C:\\\\Users\\\\gram\\\\samwon\\\\naver_news_article\\\\{keyword}\" ,file_format=\".xlsx\", \n",
    "                          save_path=f\"C:\\\\Users\\\\gram\\\\samwon\\\\naver_news_article\\\\{keyword}\\\\20220101~20220113_{keyword}_네이버_기사.xlsx\", save_format=\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09455811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import MySQLdb\n",
    "\n",
    "engine = create_engine(\"mysql+mysqldb://root:\"+\"password\"+\"@127.0.0.1:3308/mvc_project\", encoding='utf-8')\n",
    "conn = engine.connect()\n",
    "\n",
    "for keyword in keywords:\n",
    "    news = pd.read_excel(f\"C:\\\\Users\\\\gram\\\\samwon\\\\naver_news_article\\\\{keyword}\\\\20220101~20220113_{keyword}_네이버_기사.xlsx\") \n",
    "    news.to_sql(name='naver_news', con=conn, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ca8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
